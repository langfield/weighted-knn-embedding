'''
Common text preprocessing methods
'''

import re
import sys
from denis.common import util
from denis.common.replacer import replacer

_to_remove = [
    '.', ',', '!', '?', ':', ';', '>', '<',
    '"', "'", '(', ')', '{', '}', '[', ']',
    '\\', '--', '`',
]
_to_substitute = util.flatten([_to_remove, [
    '-'
]])
_removal_pattern = replacer.prepare(_to_remove, onlyAtEnds=True)
_substitution_pattern = replacer.prepare(_to_substitute, onlyAtEnds=False)

_digit_normalizers = { 
    r'^[0-9]{1,}(\.[0-9]{1,}){0,1}$': '[DIGITS]', 
    r'^\$[0-9]{1,}(\.[0-9]{1,}){0,1}$': '[MONEY]'
}

_nonxml_remover = "[^\\u0009\\u000A\\u000D\\u0020-\\uD7FF\\uE000-\\uFFFD\uD800\uDC00-\uDBFF\uDFFF]"

def tokenize(line, clean=True, tolower=True, splitwords=False):
    tokens = line.strip().split()
    if clean:
        cleanTokens = []
        for token in tokens:
            token = token.strip()
            # only force UTF-8 encoding if still in Python 2
            if sys.version[0] == '2':
                token = token.encode('utf-8')
            token = replacer.remove(_removal_pattern, token)
            if tolower: token = token.lower()
            if splitwords:
                token = replacer.suball(_substitution_pattern, ' ', token)
                cleanTokens.extend(token.split())
            else:
                cleanTokens.append(token)
        tokens = cleanTokens
    return tokens

def normalizeNumeric(text):
    if type(text) == str:
        tokens = text.split()

    normalized = []
    for t in tokens:
        for (ptrn, sub) in _digit_normalizers.items():
            t = re.sub(ptrn, sub, t)
        normalized.append(t)

    if type(text) == str:
        return ' '.join(normalized)
    else:
        return normalized

def removeNonXML(text):
    return re.sub(_nonxml_remover, '', text)

def clean(line, tolower=True, splitwords=False):
    '''Tokenizes input line and returns it as space-separated string
    '''
    return ' '.join(tokenize(line, clean=True, tolower=tolower, splitwords=splitwords))
